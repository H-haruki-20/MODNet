{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H-haruki-20/MODNet/blob/main/MODNet_WebCam_Based_Video_Matting_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he2gSkd1_Hab"
      },
      "source": [
        "# MODNet - WebCam-Based Portrait Video Matting Demo\n",
        "\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/pdf/2011.11961.pdf)          [![GitHub stars](https://img.shields.io/github/stars/ZHKKKe/MODNet?style=social)](https://github.com/ZHKKKe/MODNet)\n",
        "\n",
        "<p align=\"justify\">This is a <b>WebCam-Based Portrait Video Matting Demo</b> of our paper ''<a href=\"https://arxiv.org/pdf/2011.11961.pdf\">Is a Green Screen Really Necessary for Real-Time Portrait Matting?</a>''. We propose a trimap-free MODNet for portrait matting in real time (on a single GPU). If you want to perform matting for portrait images, please refer to our <b>Portrait Image Matting Demo</b> [<a href=\"https://colab.research.google.com/drive/1GANpbKT06aEFiW-Ssx0DQnnEADcXwQG6?usp=sharing\">colab</a>].\n",
        "\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/ZHKKKe/MODNet/develop/doc/gif/image_matting_demo.gif\" width=\"40%\"> -->\n",
        "\n",
        "<p align=\"justify\"> We use ~400 unlabeled video clips (divided into ~50,000 unlabeled frames) downloaded from the internet to perform SOC to adapt MODNet to the video domain. Nonetheless, <b><font color='#FF000'>due to insufficient labeled training data (~3k labeled foregrounds), our model may still make errors in portrait semantics estimation under challenging scenes</font></b>.\n",
        "\n",
        "Please run the code step by step to call your WebCam for real-time portrait video matting.\n",
        "Note that <b><font color='#FF000'>due to Colab resource limitations, the fps of this demo is very low</font></b> (It is also based on your network status). If you have an Ubuntu system with WebCam, please consider trying our <a href=\"https://github.com/ZHKKKe/MODNet/tree/master/demo/video_matting\">offline demo</a> to get a higher <i>fps</i>.\n",
        "\n",
        "For a better experience, please:\n",
        "\n",
        "*   use a supported browser (Google Chrome is recommended) and allow third-party cookies (used to call WebCam)\n",
        "*   make sure the network is in good condition\n",
        "*   make sure the portrait and background are distinguishable, <i>i.e.</i>, are not similar\n",
        "*   run in soft and bright ambient lighting\n",
        "*   do not be too close or too far from the WebCam\n",
        "*   do not move too fast\n",
        "\n",
        "By the way, if you have any suggestions on improving the efficiency of executing JS scripts in Colab, <i>i.e.</i>, to make this demo have a higher <i>fps</i>, please let me know. Thanks in advance!\n",
        "</p>\n",
        "\n",
        "### **Let's start!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4qPYwklGk2b"
      },
      "source": [
        "## 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEeBUunUHAnp"
      },
      "source": [
        "<p align=\"justify\">In the top menu of this session, select <b>Runtime -> Change runtime type</b>, and set <b>Hardware Accelerator</b> to <b>GPU</b>.</p>\n",
        "\n",
        "<p align=\"justify\">Clone the repository, and download the pre-trained model:</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQuJjy_UKpvW",
        "outputId": "da14d6cf-0c8d-4d59-cad7-47a984360bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'MODNet'...\n",
            "remote: Enumerating objects: 276, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 276 (delta 0), reused 1 (delta 0), pack-reused 273\u001b[K\n",
            "Receiving objects: 100% (276/276), 60.77 MiB | 11.55 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "/content/MODNet\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Nf1ZxeJZJL8Qx9KadcYYyEmmlKhTADxX\n",
            "To: /content/MODNet/pretrained/modnet_webcam_portrait_matting.ckpt\n",
            "100% 26.3M/26.3M [00:00<00:00, 33.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# clone the repository\n",
        "%cd /content\n",
        "if not os.path.exists('MODNet'):\n",
        "  !git clone https://github.com/ZHKKKe/MODNet\n",
        "%cd MODNet/\n",
        "\n",
        "# dowload the pre-trained ckpt for video matting\n",
        "pretrained_ckpt = 'pretrained/modnet_webcam_portrait_matting.ckpt'\n",
        "if not os.path.exists(pretrained_ckpt):\n",
        "  !gdown --id 1Nf1ZxeJZJL8Qx9KadcYYyEmmlKhTADxX \\\n",
        "          -O pretrained/modnet_webcam_portrait_matting.ckpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GftrpmnoHGCV"
      },
      "source": [
        "Define JS script to call WebCam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prBcl0v0KvR2"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import base64\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "\n",
        "def prepare_webcam():\n",
        "  display(\n",
        "    Javascript('''\n",
        "      var div = null;\n",
        "      var video;\n",
        "      var stream;\n",
        "      var imgElement;\n",
        "      var captureCanvas;\n",
        "      var pendingResolve = null;\n",
        "\n",
        "      async function onAnimationFrame() {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "        if (pendingResolve) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          var result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "\n",
        "          var lp = pendingResolve;\n",
        "          pendingResolve = null;\n",
        "          lp(result);\n",
        "        }\n",
        "      }\n",
        "\n",
        "      async function initWebCam() {\n",
        "        div = document.createElement('div');\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        video = document.createElement('video');\n",
        "        video.style.display = 'inline-block';\n",
        "        stream = await navigator.mediaDevices.getUserMedia(\n",
        "            {video: { facingMode: \"environment\"}});\n",
        "        div.appendChild(video);\n",
        "\n",
        "        imgElement = document.createElement('img');\n",
        "        imgElement.style.display = 'inline-block';\n",
        "        div.appendChild(imgElement);\n",
        "\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        captureCanvas = document.createElement('canvas');\n",
        "        captureCanvas.width = 640;\n",
        "        captureCanvas.height = 480;\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      async function processFrame(fgFrame) {\n",
        "        if (div == null) {\n",
        "          stream = await initWebCam();\n",
        "        }\n",
        "\n",
        "        if (fgFrame != \"\") {\n",
        "          var videoRect = video.getClientRects()[0];\n",
        "          imgElement.style.top = videoRect.top + \"px\";\n",
        "          imgElement.style.left = videoRect.left + \"px\";\n",
        "          imgElement.style.width = videoRect.width + \"px\";\n",
        "          imgElement.style.height = videoRect.height + \"px\";\n",
        "          imgElement.src = fgFrame;\n",
        "        }\n",
        "\n",
        "        return await new Promise(\n",
        "          function(resolve, reject) {pendingResolve = resolve;});\n",
        "      }\n",
        "    ''')\n",
        "  )\n",
        "\n",
        "def process_frame(fg_frame):\n",
        "  return eval_js('processFrame(\"{}\")'.format(fg_frame))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nze8eWbfHaoF"
      },
      "source": [
        "Load the pre-trained MODNet and define the inference code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pmCZ-kt4dU-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import PIL\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from src.models.modnet import MODNet\n",
        "\n",
        "\n",
        "modnet = MODNet(backbone_pretrained=False)\n",
        "modnet = nn.DataParallel(modnet).cuda()\n",
        "modnet.load_state_dict(torch.load(pretrained_ckpt))\n",
        "modnet.eval()\n",
        "\n",
        "\n",
        "torch_transforms = transforms.Compose(\n",
        "  [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "def modnet_matting(modnet, im_frame):\n",
        "  im_bytes = base64.b64decode(im_frame.split(',')[1])\n",
        "  im_PIL = PIL.Image.open(io.BytesIO(im_bytes))\n",
        "  im_np = np.asarray(im_PIL)\n",
        "\n",
        "  im_tensor = torch_transforms(im_PIL)\n",
        "  im_tensor = im_tensor[None, :, :, :].cuda()\n",
        "\n",
        "  _, _, matte_tensor = modnet(im_tensor, True)\n",
        "  matte_tensor = matte_tensor.repeat(1, 3, 1, 1)\n",
        "  matte_np = matte_tensor[0].data.cpu().numpy().transpose(1, 2, 0)\n",
        "  fg_np = matte_np * im_np + (1 - matte_np) * np.full(im_np.shape, 255.0)\n",
        "  fg_PIL = PIL.Image.fromarray(np.uint8(fg_np))\n",
        "\n",
        "  io_buffer = io.BytesIO()\n",
        "  fg_PIL.save(io_buffer, format='jpeg')\n",
        "  fg_frame = 'data:image/jpeg;base64,{}'.format(\n",
        "      (str(base64.b64encode(io_buffer.getvalue()), 'utf-8')))\n",
        "  return fg_frame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdYVB5CiJnS7"
      },
      "source": [
        "## 2. Run Video Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "GJ3B7Wbv4f_x",
        "outputId": "1c340fb0-9151-4788-e6a6-1ac278f508fa"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "      var div = null;\n",
              "      var video;\n",
              "      var stream;\n",
              "      var imgElement;\n",
              "      var captureCanvas;\n",
              "      var pendingResolve = null;\n",
              "\n",
              "      async function onAnimationFrame() {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "        if (pendingResolve) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          var result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
              "\n",
              "          var lp = pendingResolve;\n",
              "          pendingResolve = null;\n",
              "          lp(result);\n",
              "        }\n",
              "      }\n",
              "\n",
              "      async function initWebCam() {\n",
              "        div = document.createElement('div');\n",
              "        document.body.appendChild(div);\n",
              "\n",
              "        video = document.createElement('video');\n",
              "        video.style.display = 'inline-block';\n",
              "        stream = await navigator.mediaDevices.getUserMedia(\n",
              "            {video: { facingMode: \"environment\"}});\n",
              "        div.appendChild(video);\n",
              "\n",
              "        imgElement = document.createElement('img');\n",
              "        imgElement.style.display = 'inline-block';\n",
              "        div.appendChild(imgElement);\n",
              "\n",
              "        video.srcObject = stream;\n",
              "        await video.play();\n",
              "\n",
              "        captureCanvas = document.createElement('canvas');\n",
              "        captureCanvas.width = 640;\n",
              "        captureCanvas.height = 480;\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      async function processFrame(fgFrame) {\n",
              "        if (div == null) {\n",
              "          stream = await initWebCam();\n",
              "        }\n",
              "\n",
              "        if (fgFrame != \"\") {\n",
              "          var videoRect = video.getClientRects()[0];\n",
              "          imgElement.style.top = videoRect.top + \"px\";\n",
              "          imgElement.style.left = videoRect.left + \"px\";\n",
              "          imgElement.style.width = videoRect.width + \"px\";\n",
              "          imgElement.style.height = videoRect.height + \"px\";\n",
              "          imgElement.src = fgFrame;\n",
              "        }\n",
              "\n",
              "        return await new Promise(\n",
              "          function(resolve, reject) {pendingResolve = resolve;});\n",
              "      }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-800720fca489>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# matting by MODNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# NOTE: matting inference is fast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mfg_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodnet_matting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-dcffe3c6e9c9>\u001b[0m in \u001b[0;36mmodnet_matting\u001b[0;34m(modnet, im_frame)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodnet_matting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mim_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mim_PIL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mim_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_PIL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
          ]
        }
      ],
      "source": [
        "# prepare WebCam\n",
        "prepare_webcam()\n",
        "\n",
        "# main loop\n",
        "fg_frame = ''\n",
        "while True:\n",
        "  # show the processed frame and capture a new frame\n",
        "  # TODO: this step is very slow!\n",
        "  frame = process_frame(fg_frame)\n",
        "  # matting by MODNet\n",
        "  # NOTE: matting inference is fast\n",
        "  fg_frame = modnet_matting(modnet, frame)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}